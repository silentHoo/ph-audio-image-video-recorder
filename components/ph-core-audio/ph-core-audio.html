<link rel="import" href="../polymer/polymer.html">

<!-- mixins for shared functionality -->
<link rel="import" href="../ph-polymer-mixins/mixins-helper.html">
<link rel="import" href="../ph-polymer-mixins/mixins-core.html">
<link rel="import" href="../ph-polymer-mixins/mixins-audiovideo.html">

<!--
This component is a wrapper around the Web Audio API and provides a recording functionality with the used Audio
Recorder developed by Matt Diamond (Recorder.js).

This component gives basic functionality to record from a given `getUserMedia` stream and visualize the audio frequency
with vertical bars. It's possible to adjust the gain level of the stream node. The visualizer will expand to the whole
parent height and width and will show 32 frequency bars.

@group Core Elements
@element ph-core-audio
@homepage https://silenthoo.github.io/ph-core-audio
@demo https://silenthoo.github.io/ph-core-audio/demo.html
-->

<!--
Audio Recorder: Recorder.js

Copyright © 2013 Matt Diamond

Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated
documentation files (the "Software"), to deal in the Software without restriction, including without limitation
the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to
permit persons to whom the Software is furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all copies or substantial portions of the
Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE
WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR
COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR
OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
-->
<script src="../Recorderjs/recorder.js"></script>

<polymer-element name="ph-core-audio" constructor="PhCoreAudio" attributes="visualizeAudio initialGain gain barSpacing recorderWorkerFile">
  <template>
    <link rel="stylesheet" href="ph-core-audio.css">

    <template if="{{ visualizeAudio }}">
      <div id="audioVisualizer"></div>
    </template>
  </template>

  <script>
    (function () {

      Polymer(Polymer.mixin({
        publish: {
          /**
           * If set the component will visualize the input audio signal with 32 vertical bars.
           *
           * @attribute visualizeAudio
           * @type Boolean
           * @default true
           */
          visualizeAudio: true,

          /**
           * Sets the initial gain value to the given value. Possible values are floats between 0.0 and 1.0
           *
           * @attribute initialGain
           * @type Float
           * @default 0.8
           */
          initialGain: 0.8,

          /**
           * Indicates the current gain level. This value can always be updated, even when in recording.
           *
           * @attribute gain
           * @type Float
           * @default 1.0
           */
          gain: 1.0,

          /**
           * The spacing each bar (in Pixels) will have between each other.
           *
           * @attribute barSpacing
           * @type Integer
           * @default 2
           */
          barSpacing: 2,
          
          
          /**
           * The filepath of the recorderworker of Recorder.js. Depends on your bower configuration and is needed to
           * use webworkers.
           *
           * @attribute recorderWorkerFile
           * @type String
           * @default '../Recorderjs/recorderWorker.js'
           */
          recorderWorkerFile: '../Recorderjs/recorderWorker.js'
        },


        _audioContext: null,      // stores the Web Audio API context
        _userMediaStream: null,   // the consumer provided media stream
        _audioRecorder: null,     // Matt Diamonds Recorder.js
        _audioBlob: null,         // stores the audioblob (wave format) generated by Recorder.js
        _volumeFullNode: null,    // the node Recorder.js will record from
        _analyserNode: null,      // the node to extract information to visualize the bars

        /**
         * Returns `true` if the `Web Audio API` is supported by the current used browser. False otherwise.
         *
         * @return {Boolean} true if the browser is supported, false otherwise.
         */
        isSupported: function() {
          return (window.AudioContext || window.webkitAudioContext);
        },

        /**
         * The user must initially provide a getUserMedia stream to record from. For an example see
         * https://developer.mozilla.org/en-US/docs/NavigatorUserMedia.getUserMedia
         * You could also use the `ph-core-usermedia` component to get a better abstraction.
         *
         * @param userMediaStream
         */
        init: function(userMediaStream) {
          if (!this.isSupported()) {
            throw new this.CoreException('Sorry, your current browser does not support the Web Audio API :(');
          }

          this._userMediaStream = userMediaStream;
          this._initContext();

          if (this.visualizeAudio) {
            this._initVisualizer();
          }
        },

        /**
         * This method invokes the given callback with the audioBlob.
         *
         * @param {function} callback The function which will be called after the audio wave file is generated.
         * @return {boolean} Returns false, if the caller invokes the function while the recorder is recording. True
         * otherwise.
         */
        getRecordedBlob: function(callback) {
          if (this._state != this._stateEnum.STOPPED) {
            return false;
          }

          this._audioRecorder.exportWAV((function(audioBlob) {
            // Recorder.forceDownload(audioBlob, 'myRecordedAudio.wav');
            this._audioBlob = audioBlob;
            callback(this._audioBlob);
          }).bind(this));

          return true;
        },

        // This method os invoked if the `gain` value (published property) changes and updates the Web Audio Context
        // node gain level.
        gainChanged: function(event, detail, sender) {
          if (this._volumeFullNode) {
            this._volumeFullNode.gain.value = detail;
          }
        },

        // ===== private methods =====

        // state machine
        _stateChanged: function() {
          switch (this._state) {
            case this._stateEnum.RECORDING:
              this._audioBlob = null; // reset blob
              this._audioRecorder.clear();
              this._audioRecorder.record();
              break;
            case this._stateEnum.STOPPED:
              this._audioRecorder.stop();
              break;
            default:
              break;
          }
        },

        // initializes the audio context. See source code below for more comments and details.
        _initContext: function() {
          window.AudioContext = window.AudioContext || window.webkitAudioContext;

          if (!window.AudioContext) {
            console.log('AudioContext API is NOT supported in your browser :(');
            return;
          }

          // very good introduction into Web Audio API (AudioContext):
          // -> http://ianreah.com/2013/02/28/Real-time-analysis-of-streaming-audio-data-with-Web-Audio-API.html
          // @todo: only construct a new audiocontext when we really need them, otherwise we could run out of
          // the maximum reached hardware contexts allowed by the specific browser. It seems that Chrome (40) allows 6.
          this._audioContext = new AudioContext();

          // The construction of the audio pipe is based on audio nodes:

          // [1] audioSourceNode: This is the live signal of the webcam + microphone
          // [2] volumeFullNode: This is the first node connected to [1], gain level at 1.0
          // [3] analyserNode: This is the node to visualize the audio stream.
          // [4] volumeZeroNode: This is the next node connected to [2], gain level at 0.0

          // audioSourceNode -> volumeFullNode -> analyserNode -> volumeZeroNode -> destination (loud speakers)
          //                          ^
          //                   RecorderJS records
          //               with volume level 1.0 here

          // [1]: Create an AudioNode from the stream.
          var audioSourceNode = this._audioContext.createMediaStreamSource(this._userMediaStream);

          // [2]: Create a control node to set the gain level to a high value.
          this._volumeFullNode = this._audioContext.createGain();
          this._volumeFullNode.gain.value = this.initialGain;

          // [3]: Create an AnalyserNode to visualize the live stream audio data.
          //      See more at https://developer.mozilla.org/en-US/docs/Web/API/AnalyserNode
          this._analyserNode = this._audioContext.createAnalyser();
          this._analyserNode.fftSize = 64; // size of the Fast Fourier Transformation

          // [4]: Create a control node to mute the gain level to not loop the
          //      microphone input back to the speakers.
          var volumeZeroNode = this._audioContext.createGain();
          volumeZeroNode.gain.value = 0.0;

          // *** Recording ***
          this._audioRecorder = new Recorder(this._volumeFullNode, {
            workerPath: this.recorderWorkerFile
          });

          // *** Wiring all together ***
          // Connect [1] with [2]
          audioSourceNode.connect(this._volumeFullNode);
          // Connect [2] with [3]
          this._volumeFullNode.connect(this._analyserNode);
          // Connect [3] with [4]
          this._analyserNode.connect(volumeZeroNode);
          // Connect [4] with output device
          volumeZeroNode.connect(this._audioContext.destination);
        },

        // This code initializes the bar graphs overlay for the live video stream. The code is based on and inspired by
        // http://ianreah.com/2013/02/28/Real-time-analysis-of-streaming-audio-data-with-Web-Audio-API.html
        // Thank you Ian Reah ;)
        _initVisualizer: function() {
          this.async(function () {
            var frequCount        = this._analyserNode.frequencyBinCount;
            var frequencyData     = new Uint8Array(frequCount);
            var containerWidth    = this._widthOf(this.$.audioVisualizer);
            var totalSpacingSize  = this.barSpacing * frequCount;
            var barWidth          = (containerWidth - totalSpacingSize) / frequCount;

            // prepare bars
            var visualBars = '';
            for (var i = 0; i < frequCount; i++) {
              var marginLeft = i == 0 ? this.barSpacing / 2 : this.barSpacing;
              visualBars += '<div style="margin-left:' + marginLeft + 'px; width: ' + barWidth + 'px"></div>';
            }
            this.$.audioVisualizer.innerHTML = visualBars;

            // update the visualization
            this.async(function () {
              // Get the frequency data and update the visualisation
              var analyser = this._analyserNode;
              var bars = [].slice.call(this._select("#audioVisualizer > div"));

              var lastTime = Date.now();

              function update() {
                requestAnimationFrame(update);
                  analyser.getByteFrequencyData(frequencyData);

                  bars.forEach(function (bar, index) {
                    bar.style.height = frequencyData[index] + 'px';
                  });
              };

              update();
            });
          });
        }
      }, mixinsCore, mixinsHelper, mixinsAudioVideo));

    })();
  </script>
</polymer-element>
